{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "api_key=os.environ.get('GOOGLE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model = 'gemini-1.5-flash-001', temperature=0.9, api_key=api_key)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a chatbot having a conversation with a human.\"),\n",
    "    SystemMessage(content=\"You respond only in German.\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{content}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "while True:\n",
    "    content = input(\"Your prompt: \")\n",
    "    if content.lower() in ['quit', 'q', 'exit', 'bye']:\n",
    "        print('Goodbye!')\n",
    "        break\n",
    "    response = chain.invoke({'content': content})\n",
    "    print(response)\n",
    "    print('-' *50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Chat Memory Using ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon Musk is a  **technological entrepreneur, investor, and engineer** who has made a name for himself in several industries. He's known for founding companies like:\n",
      "\n",
      "* **Tesla**, a leading electric vehicle and clean energy company.\n",
      "* **SpaceX**, a private aerospace manufacturer and space transportation services company.\n",
      "* **Neuralink**, a neurotechnology company developing brain-computer interfaces.\n",
      "* **The Boring Company**, a tunneling and infrastructure company.\n",
      "\n",
      "He's also known for his ambitious goals and visionary ideas, often aimed at solving some of the world's most pressing problems. \n",
      "\n",
      "What would you like to know more about him? ðŸ˜Š \n",
      "--------------------------------------------------\n",
      "Elon Musk was born on June 28, 1971. As of today, October 26, 2023, he is **52 years old**. \n",
      "\n",
      "Is there anything else you'd like to know about him? ðŸ˜Š \n",
      "--------------------------------------------------\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import HumanMessagePromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "api_key=os.environ.get('GOOGLE_API_KEY')\n",
    "\n",
    "# Initialize LLM (Google Generative AI)\n",
    "llm = ChatGoogleGenerativeAI(model='gemini-1.5-flash-001', temperature=0.9, api_key=api_key)\n",
    "\n",
    "# Initialize memory to store conversation history\n",
    "memory = ConversationBufferMemory(memory_key='history', return_messages=True)  # Use 'history' instead of 'chat_history'\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"content\"],  # Use 'input_variables' instead of 'input_variable'\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a chatbot having a conversation with a human.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),  # 'history' to match memory_key\n",
    "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize the chain\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "while True:\n",
    "    content = input(\"Your prompt: \")\n",
    "    if content.lower() in ['quit', 'q', 'exit', 'bye']:\n",
    "        print('Goodbye!')\n",
    "        break\n",
    "\n",
    "    # Run the chain with content only (memory handles chat history automatically)\n",
    "    response = chain.invoke({'content': content})\n",
    "\n",
    "    # Print the response\n",
    "    print(response['text'])  # Extract text from response\n",
    "    print('-' * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Chat Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salman Khan is a well-known Indian actor, film producer, and television personality. He's a big name in Bollywood, the Hindi-language film industry, and has starred in many successful movies. \n",
      "\n",
      "Do you want to know more about him? For example, you could ask:\n",
      "\n",
      "*  **What are some of his most famous movies?**\n",
      "*  **What awards has he won?**\n",
      "*  **What is his personal life like?**\n",
      "\n",
      "Just let me know! ðŸ˜Š \n",
      "--------------------------------------------------\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema import SystemMessage, HumanMessage\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import HumanMessagePromptTemplate, ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory, FileChatMessageHistory\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "api_key=os.environ.get('GOOGLE_API_KEY')\n",
    "\n",
    "# Initialize LLM (Google Generative AI)\n",
    "llm = ChatGoogleGenerativeAI(model='gemini-1.5-flash-001', temperature=0.9, api_key=api_key)\n",
    "\n",
    "chat_history = FileChatMessageHistory('chat_history.json')\n",
    "\n",
    "# Initialize memory to store conversation history\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key='history',\n",
    "    return_messages=True,\n",
    "    chat_memory=chat_history)  # Use 'history' instead of 'chat_history'\n",
    "\n",
    "# Define the prompt template\n",
    "prompt = ChatPromptTemplate(\n",
    "    input_variables=[\"content\"],  # Use 'input_variables' instead of 'input_variable'\n",
    "    messages=[\n",
    "        SystemMessage(content=\"You are a chatbot having a conversation with a human.\"),\n",
    "        MessagesPlaceholder(variable_name=\"history\"),  # 'history' to match memory_key\n",
    "        HumanMessagePromptTemplate.from_template(\"{content}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize the chain\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "while True:\n",
    "    content = input(\"Your prompt: \")\n",
    "    if content.lower() in ['quit', 'q', 'exit', 'bye']:\n",
    "        print('Goodbye!')\n",
    "        break\n",
    "\n",
    "    # Run the chain with content only (memory handles chat history automatically)\n",
    "    response = chain.invoke({'content': content})\n",
    "\n",
    "    # Print the response\n",
    "    print(response['text'])  # Extract text from response\n",
    "    print('-' * 50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
